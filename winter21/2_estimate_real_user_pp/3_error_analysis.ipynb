{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from pymongo import UpdateOne\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from exploration.config import mongo_inst\n",
    "from mlpp.data_collection.sample import osuDumpSampler\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(port=27017)\n",
    "top_db = mongo_inst[\"osu_top_db\"]\n",
    "user_ids = list(map(lambda c: c['_id'], top_db['osu_user_stats'].find({}, {})))\n",
    "\n",
    "big_user_ids = pickle.load(open(\"big_user_file.obj\", \"rb\"))\n",
    "time_comparison = pickle.load(open(\"overlap_time_big_user_file.obj\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the users that don't have the time overlap between real pp and est pp\n",
    "bad_users = []\n",
    "\n",
    "for user in time_comparison:\n",
    "    if time_comparison[user][\"start\"] >= time_comparison[user][\"end\"]:\n",
    "        bad_users.append(user)\n",
    "        \n",
    "big_user_ids = [ele for ele in big_user_ids if ele not in bad_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe of error comparison\n",
    "df = pd.read_csv(\"error_df\")\n",
    "df.columns = [\"user_id\",\"mse\",\"area\",\"dtw\"]\n",
    "df[\"sqrt_mse\"] = df['mse']\n",
    "df[\"sqrt_mse\"] = np.sqrt(df[\"sqrt_mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rank score\n",
    "user_rank_score = {}\n",
    "for i in big_user_ids:\n",
    "    user_rank_score[i] = list(top_db.osu_user_stats.find({\"_id\": i}, {\"_id\": 0, \"rank_score\": 1}))[0][\"rank_score\"]\n",
    "rank_score_df = pd.DataFrame({\"user_id\": pd.Series(user_rank_score.keys()),'rank_score':pd.Series(user_rank_score.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the means & z-scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#means of each column\n",
    "col_mean = df[[\"mse\",\"area\",\"dtw\", \"sqrt_mse\"]].mean()\n",
    "col_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize errors\n",
    "cols = list(df.columns)\n",
    "cols.remove(\"user_id\")\n",
    "\n",
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Five Number Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = list(df.columns)\n",
    "subset.remove(\"user_id\")\n",
    "subset.remove(\"mse_zscore\")\n",
    "subset.remove(\"area_zscore\")\n",
    "subset.remove(\"dtw_zscore\")\n",
    "subset.remove(\"sqrt_mse_zscore\")\n",
    "df[subset].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rank_score\n",
    "df = df.merge(rank_score_df, on = \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density of zscores of the errors using three methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20,8))\n",
    "sns.histplot(ax=axes[0], data=df, x=\"sqrt_mse_zscore\", kde = True)\n",
    "axes[0].set(xlim=(-1, 5))\n",
    "axes[0].set(xlabel='Sqrt MSE Error Z-score')\n",
    "axes[0].set(ylabel='Count')\n",
    "axes[0].set(title='Histogram of MSE')\n",
    "axes[0].axvline(0, ymax=0.9, color = \"#ee6f57\")\n",
    "\n",
    "sns.histplot(ax=axes[1], data=df, x=\"area_zscore\", kde = True)\n",
    "axes[1].set(xlim=(-1, 5))\n",
    "axes[1].set(xlabel='Area Error Z-score')\n",
    "axes[1].set(ylabel='Count')\n",
    "axes[1].set(title='Histogram of Area Error')\n",
    "axes[1].axvline(0, ymax=0.9, color = \"#ee6f57\")\n",
    "\n",
    "sns.histplot(ax=axes[2], data=df, x=\"dtw_zscore\", kde=True)\n",
    "axes[2].set(xlim=(-1, 5))\n",
    "axes[2].set(xlabel='DTW Error Z-score')\n",
    "axes[2].set(ylabel='Count')\n",
    "axes[2].set(title='Histogram of DTW')\n",
    "axes[2].axvline(0, ymax=0.9, color = \"#ee6f57\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three graphs are very skewed to the right, with a heavy majority of the z-scores resting near 0.0. The error between the real pp curve and the estimate pp curve is consistent, since many users have around the same amount of error. By looking at the graph of the first user, we can see there's a negative vertical shift before 2016-12 and a postivie vertical shift after that. The DTW error z-scores have a lower range compared to the others. \n",
    "\n",
    "Since there are a lot of errors that are negative, it indicates that a lot of the raw scores are below the mean error. There are outliers that are much greater than the mean error at the right tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscore errors by rank_score\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(15,6))\n",
    "\n",
    "ax1.scatter(df['rank_score'], df['sqrt_mse_zscore'], marker = \"x\")\n",
    "ax1.set_title(\"sqrt_mse_zscore vs rank_score\")\n",
    "ax1.set_xlabel('rank_score')\n",
    "ax1.set_ylabel('sqrt_mse_zscore')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.scatter(df['rank_score'], df['area_zscore'], marker = \"x\")\n",
    "ax2.set_title('area_zscore vs rank_score')\n",
    "ax2.set_xlabel('rank_score')\n",
    "ax2.set_ylabel('area_zscore')\n",
    "ax2.grid(True)\n",
    "\n",
    "ax3.scatter(df['rank_score'], df['dtw_zscore'], marker = \"x\")\n",
    "ax3.set_title('dtw_zscore vs rank_score')\n",
    "ax3.set_xlabel('rank_score')\n",
    "ax3.set_ylabel('dtw_zscore')\n",
    "ax3.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scattor plots, we can see there are much more data points at the lower end of the rank score. As the rank score increases, there are less data points. We can see the greatest outlier lies in the middle of the rank_score.\n",
    "\n",
    "All three scatter plots show that the error between the real pp and estimate pp stays mostly consistent as you go up in rank, meaning as rank increases, the errors tend to stay the same. Rank doesn't seem to have an effect on the error. It seems that the outliers with large errors happen more often with users with low rank scores."
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "c4ee42c5-823a-42bb-b431-1d2b4d1774c6",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
